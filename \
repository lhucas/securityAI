# -*-coding:utf-8-*-

import jieba
import fasttext as fasttext
import json
import sys
import pinyin


def getStrAllAplha(str):
    return pinyin.get(str, format='strip')
def getStrFirstAplha(str):
    str=getStrAllAplha(str)
    str=str[0:1]
    return str.upper()

def run_test_reference_new(candidates,model_path='reference_model/mini.ftz'):
    jieba.load_userdict("keyword")
    print ("run test_reference_new")
    model = fasttext.load_model(model_path)
    print ("load model success")
    result_org = []
    with open(DATA_DIR, "r") as fr: 
        for line in fr:    
            result_org.append(line.strip())
    p = 0
    finily_result = []
    for text in result_org:
        result = model.predict(text)
        p += 1
        c_arr = jieba.cut(text.strip())
        sentence = " ".join(c_arr)
        num = 0
        s = sentence.split(" ")
        r_w =[]
        j = 0
        for word in s:
            result = model.predict(word)
            if result[0][0]=="__label__1" and result[1][0] > 0.5:
                r_w.append(j)
            j += 1

        for b in r_w:
            if s[b] in candidates.keys():
                #print (candidates[s[b]][0])
                s[b] = candidates[s[b]][0]
                   
            elif  getStrAllAplha(s[b]) in candidates.keys():
                print (candidates[getStrAllAplha(s[b])][0])
                s[b] = candidates[getStrAllAplha(s[b])][0]
            else:
                s[b] = ""
            
        finily_result.append("".join(s))
            

    fw = open("info", "w")
    for r in finily_result:
        fw.write(r + "\n")		
    target = json.dumps({'text': finily_result}, ensure_ascii=False)
    with open(OUT_DIR, 'w') as f:
        f.write(target)

def run_test_reference(sentences, candidate,model_path='reference_model/mini.ftz'):
	model = fasttext.load_model(model_path)
	words = []
	for texts in sentences:
		arr_txt = texts.split(" ")
		r_w =[]
		j = 0
		for word in arr_txt:
			result = model.predict(word)
			if result[0][0]=="__label__1" and result[1][0] > 0.5:
				r_w.append(j)
			j += 1
		words.append(r_w)
	finily_result = []
	for  k in range(len(sentences)):
		sentence = sentences[k]
		s = sentence.split(" ")
		for b in words[k]: 
			if s[b] in candidate.keys():
				s[b] =candidate[s[b]][0]
				print (s[b])
			else:
				s[b] = ""	
		finily_result.append("".join(s))	
	fw = open("info1", "w")
	for r in finily_result:
		fw.write(r + "\n")
		
	target = json.dumps({'text': finily_result}, ensure_ascii=False)
	with open(OUT_DIR, 'w') as f:
		f.write(target)
DATA_DIR = "/tcdata/benchmark_texts.txt"
#DATA_DIR = "ruma_example"
OUT_DIR = "adversarial.txt"

def get_test_data():
    result = []
    result_org = []
    jieba.load_userdict("keyword")
    with open(DATA_DIR, "r") as fr: 
        for line in fr: 
            c_arr = jieba.cut(line.strip())
            rr = " ".join(c_arr)    
            result_org.append(rr)
            rr = rr.lower()
            result.append(rr)
    return result, result_org


def get_candidate(file_r):
    keywords = {}
    fr = open(file_r, "r")
    for line in fr: 
        arr = line.strip().split("\t")
        keywords[arr[0].strip()] = []
        keywords[getStrAllAplha(arr[0].strip())] = []
        for i in range(1,len(arr)):
            keywords[arr[0].strip()].append(arr[i].strip())
            keywords[getStrAllAplha(arr[0].strip())].append(arr[i].strip())
    print (keywords)
    return keywords

if __name__ == "__main__":
    #data ,data_org = get_test_data()
    #tf.logging.set_verbosity(tf.logging.INFO) 
    candidate = get_candidate("candidate")
    #ads =  run_test_reference(data_org,candidate)
    ads =  run_test_reference_new(candidate)
	

